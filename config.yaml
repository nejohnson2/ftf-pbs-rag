# ============================================================
# FTF PBS RAG System Configuration
# All tunable parameters live here.
# Environment variables override the values noted below.
# ============================================================

# --- LLM (Ollama) ---
# Override with: OLLAMA_BASE_URL, OLLAMA_MODEL, OLLAMA_EMBED_MODEL
ollama:
  base_url: "http://localhost:11434"
  model: "llama3.2"
  embed_model: "nomic-embed-text"   # 768-dim; change to mxbai-embed-large for 1024-dim
  temperature: 0.1
  max_tokens: 2048
  request_timeout: 120              # seconds before LLM timeout

# --- Embeddings ---
# provider: "ollama"  → uses Ollama embed API (recommended for Heroku, no torch needed)
# provider: "sentence_transformers" → local model (requires torch; good for local dev)
embeddings:
  provider: "ollama"
  dimensions: 768                   # must match the embed model output dim
  # Sentence Transformers settings (only used when provider=sentence_transformers):
  st_model: "BAAI/bge-base-en-v1.5"
  st_device: "cpu"
  st_cache_dir: ".model_cache"

# --- Retrieval ---
retrieval:
  chunk_size: 800                   # characters per chunk
  chunk_overlap: 150                # character overlap between chunks
  top_k_semantic: 12                # candidates from vector search
  top_k_bm25: 12                    # candidates from BM25 keyword search
  top_k_reranked: 5                 # final chunks sent to LLM after reranking/fusion
  hybrid_alpha: 0.5                 # 0.0=pure BM25, 1.0=pure semantic
  # Cross-encoder reranking — requires sentence-transformers + torch.
  # Disable for Heroku unless using Docker stack.
  enable_reranker: false
  reranker_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"

# --- Query Analysis ---
# Automatic extraction of entities from user questions to pre-filter retrieval
query_analysis:
  extract_countries: true
  extract_phases: true
  extract_years: true
  extract_survey_types: true

# --- Preprocessing (run locally via scripts/preprocess.py) ---
preprocessing:
  docs_root: "Archived Population-Based Survey Reports"
  output_dir: "data/processed/markdown"
  metadata_output: "data/processed/metadata.json"
  # Text cleaning — strip noisy sections before chunking:
  remove_toc: true
  remove_references_section: true
  remove_appendices: true
  remove_survey_instruments: true
  min_paragraph_length: 80         # characters; shorter paragraphs are dropped

# --- Session ---
session:
  cookie_name: "ftf_session"
  cookie_max_age: 86400            # seconds (24 hours)
  max_history_turns: 8             # Q&A pairs kept per session

# --- Logging ---
logging:
  level: "INFO"
  log_queries: true
  log_file: "logs/queries.jsonl"   # newline-delimited JSON

# --- Database ---
# DATABASE_URL is set automatically by Heroku Postgres add-on.
# For local dev, set DATABASE_URL in .env
database:
  pool_size: 5
  max_overflow: 10

# --- App ---
app:
  title: "Feed the Future PBS Reports"
  description: "Explore Population-Based Survey reports from 20 countries"
